{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dca4823-056c-46dd-aa42-155e0efd4001",
   "metadata": {},
   "source": [
    "# Exploring `pytorch-lightning` With `xaitk-saliency` Integration\n",
    "\n",
    "This example extends the \"PyTorch Lightning DataModules\" notebook, found [here](https://github.com/Lightning-AI/tutorials/blob/publication/.notebooks/lightning_examples/datamodules.ipynb), for multi-GPU based training. Additionally, this notebook will explore integrating `xaitk-saliency` and `pytorch-lightning` to generate saliency maps for model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c8ff9-a8e1-47cd-a30d-cecd6cd7be39",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Environment Setup](#environment-setup)\n",
    "* [Introduction](#introduction)\n",
    "  * [Defining the `LitMNIST` Model](#defining-lit-mnist-model)\n",
    "    * [Accelerator Code Preparation](#accelerator-code-preparation)\n",
    "  * [Training the `LitMNIST` Model](#training-lit-mnist-model)\n",
    "    * [Selecting Devices](#selecting-accelerator-devices)\n",
    "    * [Distributed Modes](#distributed-modes)\n",
    "* [Using `DataModules`](#using-data-modules)\n",
    "  * [Defining the `MNISTDataModule`](#defining-mnist-data-module)\n",
    "  * [Defining the Dataset Agnostic `LitModel`](#defining-dataset-agnostic-lit-model)\n",
    "  * [Training `LitModel` Using the `MNISTDataModule`](#training-lit-model-for-mnist)\n",
    "  * [Defining the `CIFAR10DataModule`](#defining-cifar10-data-module)\n",
    "  * [Training `LitModel` Using the `CIFAR10DataModule`](#training-lit-model-for-cifar10)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4cb88a-db8a-4676-8e74-cb4b9bf0237d",
   "metadata": {},
   "source": [
    "## Environment Setup <a name=\"environment-setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6150f765-a61b-430d-b27c-0dbb9a04c352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing torchmetrics...\n",
      "Installing ipython...\n",
      "Installing setuptools...\n",
      "Installing torch...\n",
      "Installing pytorch-lightning...\n",
      "Installing torchvision...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -qU pip\n",
    "print(\"Installing torchmetrics...\")\n",
    "!{sys.executable} -m pip install -q 'torchmetrics>=0.7'\n",
    "print(\"Installing ipython...\")\n",
    "!{sys.executable} -m pip install -q ipython[notebook]\n",
    "print(\"Installing setuptools...\")\n",
    "!{sys.executable} -m pip install -q 'setuptools==59.5.0'\n",
    "print(\"Installing torch...\")\n",
    "!{sys.executable} -m pip install -q 'torch>=1.8'\n",
    "print(\"Installing pytorch-lightning...\")\n",
    "!{sys.executable} -m pip install -q 'pytorch-lightning>=1.4'\n",
    "print(\"Installing torchvision...\")\n",
    "!{sys.executable} -m pip install -q torchvision\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df90551-63c2-4dec-920f-c1fae041810f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "First, we'll go over a regular `LightningModule` implementation without the use of a  `LightningDataModule`. \n",
    "\n",
    "A `LightningModule` _is_ a PyTorch `nn.Module`, with additional features.\n",
    "\n",
    "Note: If the `ddp_notebook` or `ddp_fork` distributed mode is used, GPU operations such as moving tensors to the GPU or calling `torch.cuda` functions before invoking `Trainer.fit` are not allowed so we will avoid doing so now (more on distributed modes later)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cc9a40-6d3b-49d1-9d3a-2c6e4c8c8186",
   "metadata": {},
   "source": [
    "### Imports and Setup <a name=\"imports-and-setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d0b5b3-f7fd-4b14-bb37-e26abcfd5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchvision import transforms\n",
    "\n",
    "# Note you must have torchvision installed for this example\n",
    "from torchvision.datasets import CIFAR10, MNIST\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "#BATCH_SIZE = 256 if torch.cuda.is_available() else 64\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e3901f-a72e-4205-acc8-0a37606a52bf",
   "metadata": {},
   "source": [
    "### Defining the `LitMNIST` Model <a name=\"defining-lit-mnist-model\"></a>\n",
    "\n",
    "The following code reuses a `LightningModule` from `pytorch-lightning`'s [hello world tutorial](https://github.com/Lightning-AI/tutorials/blob/publication/.notebooks/lightning_examples/mnist-hello-world.ipynb) that classifies MNIST Handwritten Digits.\n",
    "\n",
    "Note that dataset-specific details are hardcoded here, limiting this module's use to MNIST Data.\n",
    "\n",
    "#### [Accelerator Code Preparation <a name=\"accelerator-code-preparation\"></a>](https://pytorch-lightning.readthedocs.io/en/latest/accelerators/accelerator_prepare.html)\n",
    "\n",
    "There are some code preparation details to keep in mind to train on any accelerator without changing code, summarized here:\n",
    "\n",
    "* Remove any `.cuda()` or `.to()` calls\n",
    "* Init tensors using `type_as` and `register_buffer`\n",
    "* Remove samplers\n",
    "* Syncronize validation and test logging (add `sync_dist=True` to all `self.log` calls in the validation and test step)\n",
    "* Make models pickleable\n",
    "\n",
    "We'll make those sure we're following those guidelines here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c4a72ff-01f4-4b04-ba5a-cc7aef7e7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMNIST(LightningModule):\n",
    "    def __init__(self, data_dir=PATH_DATASETS, hidden_size=64, learning_rate=2e-4):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # We hardcode dataset specific stuff here.\n",
    "        self.data_dir = data_dir\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Build model\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(channels * width * height, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, self.num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=128)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=128)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611feb97-f047-491b-a0cb-c6563d92b3f9",
   "metadata": {},
   "source": [
    "### Training the `LitMNIST` Model <a name=\"training-lit-mnist-model\"></a>\n",
    "\n",
    "#### [Selecting Accelerator Devices <a name=\"selecting-accelerator-devices\"></a>](https://pytorch-lightning.readthedocs.io/en/latest/accelerators/gpu_basic.html)\n",
    "\n",
    "You can select devices using ranges, a list of indices or a string containing a comma separated list of device ids:\n",
    "\n",
    "```\n",
    "# Default (int) specifies how many devices to use per node\n",
    "Trainer(accelerator=\"gpu\", devices=k)\n",
    "\n",
    "# Above is equivalant to\n",
    "Trainer(accelerator=\"gpu\", devices=list(range(k)))\n",
    "\n",
    "# Specify which devices to use (don't use when running on cluster)\n",
    "Trainer(accelerator=\"gpu\", devices=[0, 1])\n",
    "\n",
    "# String equivalent\n",
    "Trainer(accelerator=\"gpu\", devices='0, 1')\n",
    "\n",
    "# To use all available devices put -1 or '-1'\n",
    "# equivalent to list(range(torch.cuda.device_count()))\n",
    "Trainer(accelerator=\"gpu\", devices=-1)\n",
    "```\n",
    "\n",
    "Supported accelerators include: cpu, gpu, tpu, ipu, hpu, mps, auto and custom accelerator instances.\n",
    "\n",
    "#### [Distributed Modes <a name=\"distributed-modes\"></a>](https://pytorch-lightning.readthedocs.io/en/latest/accelerators/gpu_intermediate.html)\n",
    "\n",
    "PyTorch Lightning supports multiple ways of doing distributed training.\n",
    "\n",
    "* Data Parallel (`strategy='dp'`) (multiple-gpus, 1 machine)\n",
    "* Distributed Data Parallel (multiple-gpus across many machines)\n",
    "  * Regular (`strategy='ddp'`)\n",
    "  * Spawn (`strategy='ddp_spawn'`) (automatically used when no mode is set but multiple devices requested)\n",
    "  * Notebook/Fork (`strategy='ddp_notebook'`) (note Windows is not supported)\n",
    "* Horovod (`strategy='horovod'`) (multi-machine, multi-gpu, configured at runtime)\n",
    "* Bagua (`strategy='bagua'`) (multiple-gpus across many machines with advanced training algorithms)\n",
    "\n",
    "__Note only `dp` and `ddp_notebook` strategies can be used in interactive environments such as notebooks__\n",
    "\n",
    "`ddp` is the usual recommended go-to strategy for its speed and stability but it can only be used with scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abc75d78-e4ae-4025-90bb-53f1a9bb0d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 55.1 K\n",
      "-------------------------------------\n",
      "55.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.1 K    Total params\n",
      "0.220     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khq.kitware.com/emily.veenhuis/miniconda3/envs/pytorch-lightning-test/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/khq.kitware.com/emily.veenhuis/miniconda3/envs/pytorch-lightning-test/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cd50ba14a44a8f9c4277606ee1012d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "model = LitMNIST()\n",
    "trainer = Trainer(\n",
    "    max_epochs=2,\n",
    "    accelerator=\"auto\",\n",
    "    devices=-1,\n",
    "    strategy=\"dp\",\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=20)],\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cda097e-f735-45dd-9158-d6cab0210f03",
   "metadata": {},
   "source": [
    "## Using `DataModules` <a name=\"using-data-modules\"></a>\n",
    "\n",
    "`DataModules` are a way of decoupling data-related hooks from the `LightningModule` to develop dataset agnostic models.\n",
    "\n",
    "### Defining the `MNISTDataModule` <a name=\"defining-mnist-data-module\"></a>\n",
    "\n",
    "There are various components to a `LightningDataModule`:\n",
    "\n",
    "* `__init__`\n",
    "  * Takes in a `data_dir` arg that points to where you have downloaded/wish to download the MNIST dataset.\n",
    "  * Defines a transform that will be applied across train, val, and test dataset splits.\n",
    "  * Defines default `self.dims`.\n",
    "* [`prepare_data()`](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#prepare-data)\n",
    "  * This is where we can download the dataset. We point to our desired dataset and ask torchvision's MNIST dataset class to download if the dataset isn't found there.\n",
    "  * __Note we do not make any state assignments in this function__ (i.e. `self.something = ...`)\n",
    "* [`setup(stage)`](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#setup)\n",
    "  * Loads in data from file and prepares PyTorch tensor datasets for each split (train, val, test).\n",
    "  * Setup expects a 'stage' arg which is used to separate logic for 'fit' and 'test'.\n",
    "  * If you don't mind loading all your datasets at once, you can set up a condition to allow for both 'fit' related setup and 'test' related setup to run whenever `None` is passed to `stage`.\n",
    "  * __Note this runs across all GPUs and it _is_ safe to make state assignments here__\n",
    "* [`x_dataloader()`](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.core.hooks.DataHooks.html#pytorch_lightning.core.hooks.DataHooks.train_dataloader)\n",
    "  * `train_dataloader()`, `val_dataloader()`, and `test_dataloader()` all return PyTorch `DataLoader` instances that are created by wrapping their respective datasets that we prepared in `setup()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c225388-ef15-44ba-ae2d-82f44226103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(LightningDataModule):\n",
    "    def __init__(self, data_dir: str = PATH_DATASETS):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dims = (1, 28, 28)\n",
    "        self.num_classes = 10\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=BATCH_SIZE)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e319b-a194-4105-8140-c493a3345c80",
   "metadata": {},
   "source": [
    "### Defining the Dataset Agnostic `LitModel` <a name=\"defining-dataset-agnostic-lit-model\"></a>\n",
    "\n",
    "Below we define the same `LitMNIST` model as before.\n",
    "\n",
    "However, this time our model has the freedom to use any input data we'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5268a40-0e3f-4239-b009-773a4d11994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(LightningModule):\n",
    "    def __init__(self, channels, width, height, num_classes, hidden_size=64, learning_rate=2e-4):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # We take in input dimensions as parameters and use those to dynamically build model.\n",
    "        self.channels = channels\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(channels * width * height, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda93333-221b-486b-8292-377e1d001408",
   "metadata": {},
   "source": [
    "### Training `LitModel` Using the `MNISTDataModule` <a name=\"training-lit-model-for-mnist\"></a>\n",
    "\n",
    "Now, we initialize and train the `LitModel` using the `MNISTDataModule`'s configuration settings and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "189be161-1110-4473-963a-f2b907750d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 55.1 K\n",
      "-------------------------------------\n",
      "55.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.1 K    Total params\n",
      "0.220     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khq.kitware.com/emily.veenhuis/miniconda3/envs/pytorch-lightning-test/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/khq.kitware.com/emily.veenhuis/miniconda3/envs/pytorch-lightning-test/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42981629c0834148abf891a75a57f937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "# Init DataModule\n",
    "dm = MNISTDataModule()\n",
    "# Init model from datamodule's attributes\n",
    "model = LitModel(*dm.dims, dm.num_classes)\n",
    "# Init trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=3,\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=20)],\n",
    "    accelerator=\"auto\",\n",
    "    devices=-1,\n",
    "    strategy=\"dp\",\n",
    ")\n",
    "# Pass the datamodule as arg to trainer.fit to override model hooks :)\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedc8924-57ca-4bf4-8a82-5e371473fde7",
   "metadata": {},
   "source": [
    "### Defining the `CIFAR10DataModule` <a name=\"defining-cifar10-data-module\"></a>\n",
    "\n",
    "Prove `LitModel` is dataset agnostic by defining a new `DataModule` for the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8722128-5269-4ad0-9014-eb1164eceb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10DataModule(LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"./\"):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dims = (3, 32, 32)\n",
    "        self.num_classes = 10\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        CIFAR10(self.data_dir, train=True, download=True)\n",
    "        CIFAR10(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            cifar_full = CIFAR10(self.data_dir, train=True, transform=self.transform)\n",
    "            self.cifar_train, self.cifar_val = random_split(cifar_full, [45000, 5000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.cifar_test = CIFAR10(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.cifar_train, batch_size=BATCH_SIZE)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.cifar_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.cifar_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f35034c-f668-401d-bdd9-5995386f2021",
   "metadata": {},
   "source": [
    "### Training `LitModel` using the `CIFAR10DataModule` <a name=\"training-lit-model-for-cifar10\"></a>\n",
    "\n",
    "This model isn't very good, so it will perform badly on the CIFAR10 dataset.\n",
    "\n",
    "The point here is that we can see our `LitModel` has no problem using a different datamodule as its input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d1d63df-059e-4b8d-9ab5-478659f15a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 855 K \n",
      "-------------------------------------\n",
      "855 K     Trainable params\n",
      "0         Non-trainable params\n",
      "855 K     Total params\n",
      "3.420     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khq.kitware.com/emily.veenhuis/miniconda3/envs/pytorch-lightning-test/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/khq.kitware.com/emily.veenhuis/miniconda3/envs/pytorch-lightning-test/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3001447424c4b6ba194760ffe1a3c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "dm = CIFAR10DataModule()\n",
    "model = LitModel(*dm.dims, dm.num_classes, hidden_size=256)\n",
    "tqdm_progress_bar = TQDMProgressBar(refresh_rate=20)\n",
    "trainer = Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"auto\",\n",
    "    devices=-1,\n",
    "    strategy=\"dp\",\n",
    "    callbacks=[tqdm_progress_bar],\n",
    ")\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74de6cfb-5502-4bac-80f9-b8721c4e9565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
