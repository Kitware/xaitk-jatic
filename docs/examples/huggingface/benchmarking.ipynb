{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3524665-f24d-472b-988b-6dc84ff5a42b",
   "metadata": {},
   "source": [
    "# Benchmarking Hugging Face Accelerate/`xaitk-saliency` Integration\n",
    "\n",
    "This notebook utilizes PyTorch's benchmarking capability, along with [`submitit`](https://github.com/facebookincubator/submitit), to anaylze the integration strategy used for Hugging Face Accelerate and `xaitk-saliency`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e9d8fe-17c8-410a-92c8-6d830b59cc4e",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Environment Setup](#environment-setup)\n",
    "* [Benchmarking](#benchmarking)\n",
    "  * [GPU Sweep](#gpu-sweep)\n",
    "  * [Mask Sweep](#mask-sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4b026b-8ae4-4e15-b044-9625cb795aef",
   "metadata": {},
   "source": [
    "## Environment Setup <a name=\"environment-setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab36330f-3e9c-4679-9b0d-27006a6be5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing xaitk-jatic...\n",
      "Installing xaitk-saliency...\n",
      "Installing smqtk-classifier...\n",
      "Installing Hugging Face datasets...\n",
      "Installing Hugging Face transformers...\n",
      "Installing Hugging Face accelerate...\n",
      "Installing submitit...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import sys  # noqa:F401\n",
    "\n",
    "!{sys.executable} -m pip install -qU pip\n",
    "print(\"Installing xaitk-jatic...\")\n",
    "!{sys.executable} -m pip install -q ../..\n",
    "print(\"Installing xaitk-saliency...\")\n",
    "!{sys.executable} -m pip install -q xaitk-saliency\n",
    "print(\"Installing smqtk-classifier...\")\n",
    "!{sys.executable} -m pip install -qU smqtk-classifier\n",
    "print(\"Installing Hugging Face datasets...\")\n",
    "!{sys.executable} -m pip install -q datasets\n",
    "print(\"Installing Hugging Face transformers...\")\n",
    "!{sys.executable} -m pip install -q transformers\n",
    "print(\"Installing Hugging Face accelerate...\")\n",
    "!{sys.executable} -m pip install -q accelerate\n",
    "print(\"Installing submitit...\")\n",
    "!{sys.executable} -m pip install -q 'submitit'\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e2857a-78d6-4a9b-a0dc-df71371c323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note PREDICT_SIZE should be >= BATCH_SIZE, due to the way Accelerate distributes data\n",
    "BATCH_SIZE = 25\n",
    "PREDICT_SIZE = 100\n",
    "MASKED_DATA_BATCH_SIZE = 128\n",
    "\n",
    "min_run_time = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97eb8c0b-42e1-4afc-8bff-093ebfca03ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Use JPEG format for inline visualizations\n",
    "%config InlineBackend.figure_format = \"jpeg\"\n",
    "\n",
    "# For \"artifact tracking\" (to compare results)\n",
    "import pickle\n",
    "from collections.abc import Iterable, Iterator\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import submitit\n",
    "import torch\n",
    "import torch.utils.benchmark as benchmark\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from accelerate.utils import set_seed\n",
    "from datasets import load_dataset\n",
    "from scipy.special import softmax\n",
    "from smqtk_classifier.interfaces.classify_image import ClassifyImage\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
    "from torchvision import transforms\n",
    "from transformers import AutoModelForImageClassification\n",
    "from typing_extensions import override\n",
    "from xaitk_saliency.impls.gen_image_classifier_blackbox_sal.rise import RISEStack\n",
    "from xaitk_saliency.impls.gen_image_classifier_blackbox_sal.slidingwindow import SlidingWindowStack\n",
    "from xaitk_saliency.interfaces.gen_image_classifier_blackbox_sal import GenerateImageClassifierBlackboxSaliency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24fad41-b3dc-4c8e-a1a6-18672bccbc88",
   "metadata": {},
   "source": [
    "The following is code from the original integration notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d84bcd-6be4-43ad-9ffc-3f3b4ac80ebf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def app(  # noqa:C901\n",
    "    saliency_generator: GenerateImageClassifierBlackboxSaliency,\n",
    "    use_accelerate: bool = True,\n",
    "    display_results: bool = False,\n",
    "    results_filepath: str = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates saliency maps for a subset of the CIFAR10 dataset using a specified model\n",
    "    and a saliency map generator. Optionally uses Hugging Face Accelerate for distributed inference.\n",
    "\n",
    "    Args:\n",
    "        saliency_generator (GenerateImageClassifierBlackboxSaliency):\n",
    "            A saliency map generator using black-box image classifiers.\n",
    "        use_accelerate (bool, optional):\n",
    "            Whether to use Hugging Face Accelerate for model and data parallelism. Defaults to True.\n",
    "        display_results (bool, optional):\n",
    "            Whether to display the generated saliency maps. Defaults to False.\n",
    "        results_filepath (str, optional):\n",
    "            Path to save the gathered saliency maps. If None, results are not saved. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        None: The function generates and optionally displays or saves saliency maps.\n",
    "    \"\"\"\n",
    "\n",
    "    class TestDataset(Dataset):\n",
    "        \"\"\"\n",
    "        A dataset for testing image classification and saliency map generation.\n",
    "\n",
    "        Attributes:\n",
    "            data (list[np.ndarray]): List of input images.\n",
    "            transform (transforms.Compose): Transformation pipeline applied to images.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, data: list[np.ndarray]) -> None:\n",
    "            \"\"\"\n",
    "            Initializes the TestDataset.\n",
    "\n",
    "            Args:\n",
    "                data (list[np.ndarray]): List of input images as NumPy arrays.\n",
    "            \"\"\"\n",
    "            self.data = data\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                    transforms.Resize((224, 224), antialias=True),\n",
    "                ],\n",
    "            )\n",
    "\n",
    "        def __getitem__(self, index: int) -> np.ndarray:\n",
    "            \"\"\"\n",
    "            Retrieves and transforms an image at the specified index.\n",
    "\n",
    "            Args:\n",
    "                index (int): Index of the image.\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: Transformed image tensor.\n",
    "            \"\"\"\n",
    "            return self.transform(self.data[index])\n",
    "\n",
    "        def __len__(self) -> int:\n",
    "            \"\"\"\n",
    "            Returns the total number of images in the dataset.\n",
    "\n",
    "            Returns:\n",
    "                int: Number of images.\n",
    "            \"\"\"\n",
    "            return len(self.data)\n",
    "\n",
    "    accelerator = None\n",
    "    if use_accelerate:\n",
    "        # For reproducability\n",
    "        set_seed(42)\n",
    "\n",
    "        # Set up the accelerator\n",
    "        accelerator = Accelerator(even_batches=False)\n",
    "\n",
    "    # Get the model\n",
    "    model_name = \"aaraki/vit-base-patch16-224-in21k-finetuned-cifar10\"\n",
    "    model = AutoModelForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "    # Predicting on a subset of the CIFAR10 Test dataset\n",
    "    ds = load_dataset(\"cifar10\", split=\"test\")\n",
    "    labels = ds.features[\"label\"].names\n",
    "    num_classes = len(labels)\n",
    "    ds_shuffle = ds.shuffle(seed=42)\n",
    "    images = ds_shuffle[0:PREDICT_SIZE][\"img\"]\n",
    "    dataloader = DataLoader(TestDataset(images), batch_size=min(BATCH_SIZE, PREDICT_SIZE))\n",
    "\n",
    "    if accelerator:\n",
    "        # Prepare the model and dataloader for use with accelerate\n",
    "        model, dataloader = accelerator.prepare(model, dataloader)\n",
    "\n",
    "    image_classifier = AccelerateClassifier(model, labels, accelerator, transform=None)\n",
    "\n",
    "    # Generate saliency maps\n",
    "    sal_maps_set = []\n",
    "    for batch in dataloader:\n",
    "        b = batch.cpu().data.numpy()\n",
    "        for img in b:\n",
    "            sal_maps = saliency_generator(np.moveaxis(img, 0, -1), image_classifier)\n",
    "            sal_maps_set.append(sal_maps)\n",
    "\n",
    "    if accelerator:\n",
    "        accelerator.wait_for_everyone()\n",
    "        t_sal_maps_set = torch.Tensor(np.array(sal_maps_set)).to(accelerator.device)\n",
    "        sal_maps_set_gathered = accelerator.gather(t_sal_maps_set)\n",
    "        sal_maps_set_gathered = sal_maps_set_gathered.data.cpu().numpy()\n",
    "    else:\n",
    "        sal_maps_set_gathered = sal_maps_set\n",
    "\n",
    "    # Plot each image in set with saliency maps\n",
    "    if display_results and (accelerator is None or accelerator.is_main_process):\n",
    "        for i in range(len(images)):\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            num_cols = np.ceil(num_classes / 2).astype(int) + 1\n",
    "            plt.subplot(2, num_cols, 1)\n",
    "            plt.imshow(images[i], cmap=\"gray\")\n",
    "            plt.xticks(())\n",
    "            plt.yticks(())\n",
    "\n",
    "            for c in range(num_cols - 1):\n",
    "                plt.subplot(2, num_cols, c + 2)\n",
    "                plt.imshow(sal_maps_set_gathered[i][c], cmap=plt.cm.RdBu, vmin=-1, vmax=1)\n",
    "                plt.xticks(())\n",
    "                plt.yticks(())\n",
    "                plt.xlabel(f\"{labels[c]}\")\n",
    "            for c in range(num_classes - num_cols + 1, num_classes):\n",
    "                plt.subplot(2, num_cols, c + 3)\n",
    "                plt.imshow(sal_maps_set_gathered[i][c], cmap=plt.cm.RdBu, vmin=-1, vmax=1)\n",
    "                plt.xticks(())\n",
    "                plt.yticks(())\n",
    "                plt.xlabel(f\"{labels[c]}\")\n",
    "\n",
    "    # Save results for comparison using a context manager\n",
    "    if results_filepath is not None and (accelerator is None or accelerator.is_main_process):\n",
    "        with open(results_filepath, \"wb\") as file:\n",
    "            pickle.dump(sal_maps_set_gathered, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665e2870-b6b4-4b5a-a13a-21ddda7524a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccelerateClassifier(ClassifyImage):\n",
    "    \"\"\"\n",
    "    A classifier that leverages a PyTorch model and the Hugging Face Accelerate library\n",
    "    to classify images with potential transformations and distributed device support.\n",
    "\n",
    "    Attributes:\n",
    "        model (torch.nn.Module): The PyTorch model used for inference.\n",
    "        labels (List[str]): The list of class labels.\n",
    "        accelerator (Accelerator): The Hugging Face Accelerate object for managing devices.\n",
    "        transform (transforms.Compose): Preprocessing transformations for input images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        labels: list[str],\n",
    "        accelerator: Accelerator = None,\n",
    "        transform: transforms.transforms.Compose = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the AccelerateClassifier.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): A PyTorch model for classification.\n",
    "            labels (List[str]): A list of class labels corresponding to model predictions.\n",
    "            accelerator (Accelerator): An optional Hugging Face Accelerator for device management.\n",
    "            transform (transforms.Compose): Optional transformations for preprocessing input images.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.accelerator = accelerator\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    @override\n",
    "    def get_labels(self) -> list[str]:\n",
    "        return self.labels\n",
    "\n",
    "    class ClassifyImagesDataset(IterableDataset):\n",
    "        \"\"\"\n",
    "        An iterable dataset that processes a stream of images for classification.\n",
    "\n",
    "        Attributes:\n",
    "            _iterable (Iterable[np.ndarray]): Iterable of input images as NumPy arrays.\n",
    "            _device (str): Target device for tensor processing (e.g., 'cpu', 'cuda').\n",
    "            _transform (transforms.Compose): Transformations applied to input images.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(\n",
    "            self,\n",
    "            iterable: Iterable[np.ndarray],\n",
    "            device: str = None,\n",
    "            transform: transforms.transforms.Compose = None,\n",
    "        ) -> None:\n",
    "            \"\"\"\n",
    "            Initializes the ClassifyImagesDataset.\n",
    "\n",
    "            Args:\n",
    "                iterable (Iterable[np.ndarray]): Iterable of input images as NumPy arrays.\n",
    "                device (str): Device to which tensors will be moved (e.g., 'cuda', 'cpu').\n",
    "                transform (transforms.Compose): Optional transformations for preprocessing.\n",
    "            \"\"\"\n",
    "            self._iterable = iterable\n",
    "            self._device = device\n",
    "            self._transform = transform\n",
    "\n",
    "        def __iter__(self) -> Iterator[torch.Tensor]:\n",
    "            \"\"\"\n",
    "            Processes images by applying transformations and moving tensors to the appropriate device.\n",
    "\n",
    "            Yields:\n",
    "                torch.Tensor: Preprocessed image tensors ready for inference.\n",
    "            \"\"\"\n",
    "            tnsfm = self._transform\n",
    "            device = self._device\n",
    "\n",
    "            for image in self._iterable:\n",
    "                image = np.moveaxis(image, -1, 0)\n",
    "                item = tnsfm(image) if tnsfm else image\n",
    "                if device:\n",
    "                    item = torch.Tensor(item).to(device)\n",
    "                yield item\n",
    "\n",
    "    @override\n",
    "    def classify_images(self, image_iter: Iterable[np.ndarray]) -> list[dict[str, float]]:\n",
    "        dataloader = DataLoader(\n",
    "            self.ClassifyImagesDataset(image_iter, self.accelerator.device if self.accelerator else None),\n",
    "            batch_size=MASKED_DATA_BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        self.model.eval()\n",
    "        results = []\n",
    "        for batch in dataloader:\n",
    "            with torch.no_grad():\n",
    "                preds = softmax(self.model(batch).logits.data.cpu().numpy(), axis=1)\n",
    "            results.extend([{la: p for p, la in zip(pred, self.labels)} for pred in preds])\n",
    "\n",
    "        return results\n",
    "\n",
    "    # Required for implementation\n",
    "    @override\n",
    "    def get_config(self) -> dict[str, Any]:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f20544-42c5-4ef1-a2bf-7126a34cc377",
   "metadata": {},
   "source": [
    "## Benchmarking <a name=\"benchmarking\"></a>\n",
    "\n",
    "We'll benchmark against (1) a varying number of GPUs and (2) a varying number of masks to see how this affects computation time.\n",
    "\n",
    "We'll first define a utility function to more easily submit jobs via submitit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cc6ab60-4b8f-4ae3-b3f0-78de999041dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_app(*args: Any, **kwargs: Any) -> None:\n",
    "    \"\"\"\n",
    "    Runs the specified app using an executor and retrieves the results.\n",
    "\n",
    "    Args:\n",
    "        app (Callable[..., Any]): The application function to run.\n",
    "        *args (Any): Positional arguments to pass to the `app` function.\n",
    "        **kwargs (Any): Keyword arguments to pass to the `app` function.\n",
    "\n",
    "    Returns:\n",
    "        None: This function launches the app as a job and retrieves its results.\n",
    "    \"\"\"\n",
    "    job = executor.submit(notebook_launcher, *args, **kwargs)\n",
    "    job.results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187896f-fc1e-401d-b761-26f70e42ce33",
   "metadata": {},
   "source": [
    "### GPU Sweep <a name=\"gpu-sweeip\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a52d51-e702-4779-aea2-ba243d182026",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_sliding_window = SlidingWindowStack(window_size=(14, 14), stride=(7, 7), threads=4)\n",
    "\n",
    "gpu_benchmark_results = []\n",
    "\n",
    "gpus = [1, 2, 4]\n",
    "\n",
    "for g in gpus:\n",
    "    label = f\"GPU Sweep ({PREDICT_SIZE} sample images)\"\n",
    "    sub_label = f\"{g} GPU\"\n",
    "\n",
    "    executor = submitit.AutoExecutor(folder=\"submitit_logs\", cluster=\"slurm\")\n",
    "    executor.update_parameters(gpus_per_node=g, slurm_partition=\"community\", slurm_account=\"xai\", timeout_min=180)\n",
    "    args = (\n",
    "        app,\n",
    "        (\n",
    "            gen_sliding_window,\n",
    "            True,  # use_accelerate\n",
    "            False,  # display_results\n",
    "            None,\n",
    "        ),  # results_filepath\n",
    "    )\n",
    "    kwargs = {\"num_processes\": g}\n",
    "    print(kwargs)\n",
    "\n",
    "    print(f\"Starting GPU sweep test: {g} GPU\")\n",
    "    gpu_benchmark_results.append(\n",
    "        benchmark.Timer(\n",
    "            stmt=\"run_app(app, *args, **kwargs)\",\n",
    "            setup=\"from __main__ import run_app\",\n",
    "            globals={\"app\": app, \"args\": args, \"kwargs\": kwargs},\n",
    "            label=label,\n",
    "            sub_label=sub_label,\n",
    "            description=\"time\",\n",
    "        ).blocked_autorange(min_run_time=min_run_time),\n",
    "    )\n",
    "\n",
    "compare = benchmark.Compare(gpu_benchmark_results)\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e885705-7c94-4ba9-b4d0-1797dc0ef04d",
   "metadata": {},
   "source": [
    "### Mask Sweep <a name=\"mask-sweep\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66eded8-89d9-463a-b4db-d85dd1efb04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting number masks test: 50 masks\n",
      "Starting number masks test: 100 masks\n",
      "Starting number masks test: 200 masks\n",
      "Starting number masks test: 400 masks\n",
      "[ Mask Sweep (100 sample images) ]\n",
      "                 |  time\n",
      "1 threads: -------------\n",
      "      50 Masks   |  14.2\n",
      "      100 Masks  |  20.2\n",
      "      200 Masks  |  36.3\n",
      "      400 Masks  |  60.3\n",
      "\n",
      "Times are in seconds (s).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mask_benchmark_results = []\n",
    "\n",
    "n_masks = [50, 100, 200, 400]\n",
    "gpus = 4\n",
    "\n",
    "for n in n_masks:\n",
    "    label = f\"Mask Sweep ({PREDICT_SIZE} sample images)\"\n",
    "    sub_label = f\"{n} Masks\"\n",
    "\n",
    "    executor = submitit.AutoExecutor(folder=\"submitit_logs\", cluster=\"slurm\")\n",
    "    executor.update_parameters(gpus_per_node=gpus, slurm_partition=\"community\", slurm_account=\"xai\", timeout_min=180)\n",
    "    kwargs = {\"num_processes\": gpus}\n",
    "\n",
    "    gen_rise_stack = RISEStack(n=n, s=8, p1=0.5, seed=0, threads=4)\n",
    "\n",
    "    print(f\"Starting number masks test: {n} masks\")\n",
    "    args = (\n",
    "        app,\n",
    "        (\n",
    "            gen_rise_stack,\n",
    "            True,  # use_accelerate\n",
    "            False,  # display_results\n",
    "            None,\n",
    "        ),  # results_filepath\n",
    "    )\n",
    "    mask_benchmark_results.append(\n",
    "        benchmark.Timer(\n",
    "            stmt=\"run_app(app, *args, **kwargs)\",\n",
    "            setup=\"from __main__ import run_app\",\n",
    "            globals={\"app\": app, \"args\": args, \"kwargs\": kwargs},\n",
    "            label=label,\n",
    "            sub_label=sub_label,\n",
    "            description=\"time\",\n",
    "        ).blocked_autorange(min_run_time=min_run_time),\n",
    "    )\n",
    "\n",
    "compare = benchmark.Compare(mask_benchmark_results)\n",
    "compare.print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xaitk-jatic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
